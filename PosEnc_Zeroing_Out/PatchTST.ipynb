{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os, sys\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append('path/to/PatchTST/PatchTST_supervised')\n",
    "os.chdir('path/to/PatchTST/PatchTST_supervised')\n",
    "\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual, test_params_flop\n",
    "from utils.metrics import metric\n",
    "from data_provider.data_factory import data_provider\n",
    "from models import Informer, Autoformer, Transformer, DLinear, Linear, NLinear, PatchTST\n",
    "from models.PatchTST import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # load parameters\n",
    "        self.enc_in = 1\n",
    "        self.seq_len = 96\n",
    "        self.label_len = 0\n",
    "        self.pred_len = 96\n",
    "        self.e_layers = 6\n",
    "        self.n_heads = 16\n",
    "        self.d_model = 128\n",
    "        self.d_ff = 256\n",
    "        self.dropout = 0.2\n",
    "        self.fc_dropout = 0.2\n",
    "        self.head_dropout = 0.0\n",
    "        self.individual = False\n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        self.padding_patch = 'end'\n",
    "        self.revin = True\n",
    "        self.affine = False\n",
    "        self.subtract_last = False\n",
    "        self.decomposition = False\n",
    "        self.kernel_size = 25\n",
    "\n",
    "        self.batch_size = 128\n",
    "        self.data = 'ETTh1'\n",
    "        self.embed = 'timeF'\n",
    "        self.freq = 'h'\n",
    "        self.root_path = 'path/to/PatchTST/PatchTST_supervised/dataset'\n",
    "        self.data_path = 'ETTh1.csv'\n",
    "        self.features = 'M'\n",
    "        self.target = 'OT'\n",
    "        self.num_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_configs = {\n",
    "    'ETTm2': {'data':'ETTm2', 'data_path': 'path/to/PatchTST/PatchTST_supervised/dataset/'+'/ETT-small/ETTm2.csv', 'enc_in': 7},\n",
    "    'Electricity': {'data':'custom', 'data_path': 'path/to/PatchTST/PatchTST_supervised/dataset/'+'/electricity/electricity.csv', 'enc_in': 321},\n",
    "    'weather': {'data':'custom', 'data_path': 'path/to/PatchTST/PatchTST_supervised/dataset/'+'/weather/weather.csv', 'enc_in': 21},\n",
    "    'traffic': {'data':'custom', 'data_path': 'path/to/PatchTST/PatchTST_supervised/dataset/'+'/traffic/traffic.csv', 'enc_in': 862},\n",
    "    'Dataset_Patch_dependent': {'data':'Dataset_Patch_dependent', 'data_path': 'path/to/PatchTST/PatchTST_supervised/dataset/'+'/traffic/traffic.csv', 'enc_in': 1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "args.seq_len = 336\n",
    "args.stride = 16\n",
    "args.e_layers = 3\n",
    "dataset = 'ETTm2'\n",
    "model_path = 'path/to/checkpoint.pth'\n",
    "\n",
    "for attr in dataset_configs[dataset].keys():\n",
    "    value = dataset_configs[dataset][attr]\n",
    "    setattr(args, attr, value)\n",
    "\n",
    "train_data, train_loader = data_provider(args, flag='train')\n",
    "vali_data, vali_loader = data_provider(args, flag='val')\n",
    "test_data, test_loader = data_provider(args, flag='test')\n",
    "\n",
    "model = Model(args).to('cuda')\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.module\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = 'cuda'\n",
    "criterion = nn.MSELoss().to(device)\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tqdm(test_loader)):\n",
    "    # for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tqdm(train_loader)):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "        outputs = model(batch_x)\n",
    "        outputs = outputs[:, -args.pred_len:, 0:]\n",
    "        batch_y = batch_y[:, -args.pred_len:, 0:].to(device)\n",
    "\n",
    "        pred = outputs.detach().cpu()\n",
    "        true = batch_y.detach().cpu()\n",
    "\n",
    "        loss = criterion(pred, true)\n",
    "\n",
    "        total_loss.append(loss)\n",
    "total_loss = np.average(total_loss)\n",
    "print('loss:', total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PosEnc Zeroing Out Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero out the positional embedding\n",
    "pos_embed_raw = model.model.backbone.W_pos\n",
    "model.model.backbone.W_pos = nn.Parameter(torch.zeros_like(model.model.backbone.W_pos) + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the positional embedding\n",
    "model.model.backbone.W_pos = pos_embed_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = 'cuda'\n",
    "criterion = nn.MSELoss().to(device)\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tqdm(test_loader)):\n",
    "    # for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tqdm(train_loader)):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "        outputs = model(batch_x)\n",
    "        outputs = outputs[:, -args.pred_len:, 0:]\n",
    "        batch_y = batch_y[:, -args.pred_len:, 0:].to(device)\n",
    "\n",
    "        pred = outputs.detach().cpu()\n",
    "        true = batch_y.detach().cpu()\n",
    "\n",
    "        loss = criterion(pred, true)\n",
    "\n",
    "        total_loss.append(loss)\n",
    "total_loss = np.average(total_loss)\n",
    "print('loss:', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(vectors):\n",
    "    norms = vectors.norm(p=2, dim=1, keepdim=True)\n",
    "    normalized_vectors = vectors / norms\n",
    "    return torch.mm(normalized_vectors, normalized_vectors.T)\n",
    "\n",
    "similarity_matrix = cosine_similarity_matrix(model.model.backbone.W_pos)\n",
    "vmax = torch.max(similarity_matrix).item()\n",
    "vmin = torch.min(similarity_matrix).item()\n",
    "print(vmax, vmin)\n",
    "\n",
    "similarity_matrix = similarity_matrix[:-1, :-1]\n",
    "from collections import defaultdict\n",
    "distance_similarity = defaultdict(list)\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    for j in range(similarity_matrix.shape[1]):\n",
    "        if i != j:\n",
    "            distance = abs(i-j)\n",
    "            distance_similarity[distance].append(similarity_matrix[i, j].item())\n",
    "distance_similarity = {k: np.mean(v) for k, v in distance_similarity.items()}\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(distance_similarity.keys(), distance_similarity.values(), color='dodgerblue', linestyle='-', linewidth=2, marker='o', markersize=6, markerfacecolor='red')\n",
    "plt.xlabel('Token Distance')\n",
    "plt.ylabel('Token Similarity')\n",
    "# plt.title('distance_similarity')\n",
    "plt.xticks(range(1, len(distance_similarity)+1))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
